{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "582a9b4d-a225-4099-9cbd-6b1411f37616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (2024.11.5)\n",
      "Requirement already satisfied: xformers==0.0.28.post2 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (0.0.28.post2)\n",
      "Requirement already satisfied: numpy in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from xformers==0.0.28.post2) (2.1.2)\n",
      "Requirement already satisfied: torch==2.5.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from xformers==0.0.28.post2) (2.5.0)\n",
      "Requirement already satisfied: filelock in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (4.11.0)\n",
      "Requirement already satisfied: networkx in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch==2.5.0->xformers==0.0.28.post2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.0->xformers==0.0.28.post2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from jinja2->torch==2.5.0->xformers==0.0.28.post2) (2.1.3)\n",
      "Found existing installation: unsloth 2024.11.5\n",
      "Uninstalling unsloth-2024.11.5:\n",
      "  Successfully uninstalled unsloth-2024.11.5\n",
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-erxf_zsf/unsloth_fca2ac83dc3b49b79106a46127e07b22\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-erxf_zsf/unsloth_fca2ac83dc3b49b79106a46127e07b22\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 4cbebe151d9c8f813e4e69be1d86a5657a44ee60\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting unsloth_zoo>=2024.11.8 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading unsloth_zoo-2024.12.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.1)\n",
      "Requirement already satisfied: tyro in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.14)\n",
      "Requirement already satisfied: transformers>=4.46.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.46.1)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.6)\n",
      "Requirement already satisfied: psutil in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.0)\n",
      "Requirement already satisfied: numpy in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.2)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.26.2)\n",
      "Requirement already satisfied: hf_transfer in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.3 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.1)\n",
      "Requirement already satisfied: torch in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: filelock in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.1)\n",
      "Requirement already satisfied: triton in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.0.1)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.11.4)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.2)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading cut_cross_entropy-24.12.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pillow (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.3)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.15.1)\n",
      "Requirement already satisfied: networkx in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hq4793/.conda/envs/lora-ft/lib/python3.11/site-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.3)\n",
      "Downloading unsloth_zoo-2024.12.1-py3-none-any.whl (60 kB)\n",
      "Downloading cut_cross_entropy-24.12.1-py3-none-any.whl (22 kB)\n",
      "Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.12.2-py3-none-any.whl size=167841 sha256=71e20f0eae054dbf1d9bedc77e8cf6d99ddd42dfc9fcdbb95a8424e73f94e74d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yu3g1r2l/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth, pillow, cut_cross_entropy, unsloth_zoo\n",
      "  Attempting uninstall: unsloth_zoo\n",
      "    Found existing installation: unsloth_zoo 2024.11.4\n",
      "    Uninstalling unsloth_zoo-2024.11.4:\n",
      "      Successfully uninstalled unsloth_zoo-2024.11.4\n",
      "Successfully installed cut_cross_entropy-24.12.1 pillow-11.0.0 unsloth-2024.12.2 unsloth_zoo-2024.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2bcae1c-5e55-4f53-849a-6d54d9d4679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support |RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit\",  \n",
    "  ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cfff10-bc0c-4b6e-b06b-9df7e6dffcce",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c89839-11a1-4268-8339-4ff814998dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA H100 PCIe. Max memory: 79.216 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e115ebae2743528f3c9b8f7599a7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit\",\n",
    "     max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d26f8a-14a6-433d-b3f4-c2c10036108f",
   "metadata": {},
   "source": [
    "## Add LoRa adapters (config for less overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280388b5-8845-403a-8de9-10eb5d768e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.2 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a5f02-efce-455f-8aa3-ed45ad5fd86f",
   "metadata": {},
   "source": [
    "## LoRa Adapters for more overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17ffcd4-8071-4771-9631-84b44b59148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.2 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # Rank for LoRA\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 32,  # Double the rank (alpha = 2 * r)\n",
    "    lora_dropout = 0,  # No dropout\n",
    "    bias = \"none\",  # Optimized for no bias\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Gradient checkpointing for memory efficiency\n",
    "    random_state = 3407,  # Random seed for reproducibility\n",
    "    use_rslora = False,  # Default LoRA (no rank stabilization)\n",
    "    loftq_config = None,  # No quantization (unless you use QLoRA)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943bee2-c01b-4cf3-916c-0797813dd1ed",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0472e2fe-d9e5-42ba-aa4e-5797d8db85a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "\n",
    "tokenizer_format = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2682258-8683-4079-83e2-1a2ea2921d37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(row_json, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n\u001b[0;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     13\u001b[0m     format_chat_template,\n\u001b[1;32m     14\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "INSTRUCTION = \"\"\"\"You are Dr. Naomi, a skilled Motivational Interviewing counselor helping clients who are working towards weight loss. \n",
    "Your goal is to boost motivation using empathy, reflection, and open-ended questions. Provide information on healthy diet and exercise only after asking for the client's consent. \n",
    "Keep responses concise, avoid incomplete numbered lists (limit to three points), and steer conversations in a positive direction if the client feels discouraged. \n",
    "Always engage with one open-ended follow-up question at a time, avoid assumptions about feelings or emotions, and never refuse to answer a question. \n",
    "Be polite, non-judgmental, and supportive of their struggles.\n",
    "\"\"\"\n",
    "def format_chat_template(row):\n",
    "    \n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction },\n",
    "               {\"role\": \"user\", \"content\": row[\"Context\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Response\"]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n",
    "len(dataset[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca0d0b46-50fa-4717-bf51-3b7055ab80f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3512"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bca286-7f32-413f-b301-129679d5d4fb",
   "metadata": {},
   "source": [
    "## SYSTEM PROMPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89632fcd-a603-4266-ad65-a3b93c259384",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\"You are Dr. Naomi, a skilled Motivational Interviewing counselor helping clients who are working towards weight loss. \n",
    "Your goal is to boost motivation using empathy, reflection, and open-ended questions. Provide information on healthy diet and exercise only after asking for the client's consent. \n",
    "Keep responses concise, avoid incomplete numbered lists (limit to three points), and steer conversations in a positive direction if the client feels discouraged. \n",
    "Always engage with one open-ended follow-up question at a time, avoid assumptions about feelings or emotions, and never refuse to answer a question. \n",
    "Be polite, non-judgmental, and supportive of their struggles.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af109ef-15fd-45cd-8eb4-18cfd4d5a038",
   "metadata": {},
   "source": [
    "## Choose dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f87dee-88ac-453f-b5ce-26dd76dbe822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS_LOCATION = \"/scr/naomi-finetune/final_dataset.json\"\n",
    "\n",
    "DS_LOCATION = \"/scr/naomi-finetune/obesity_RCT_pilot_only.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee5472-54fc-466c-b071-4b3f496e60d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Processing custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdcabb75-7a87-438a-a6f6-6ecb80efa691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries: 1114\n",
      "[{'content': 'Fine', 'role': 'user'}, {'content': 'So lots of stuff today in the questionnaries about nutrition and activity and things list that. I just want to know how do you feel about your weight?', 'role': 'assistant'}, {'content': \"I don't know.\", 'role': 'user'}, {'content': \"It's not really anything you've thought about before.\", 'role': 'assistant'}, {'content': 'Mm-hmm.', 'role': 'user'}, {'content': \"And nobody's really talked to you about your weight or needing to change it at all.\", 'role': 'assistant'}, {'content': 'Just my mom be like when we go and walk around the track.', 'role': 'user'}, {'content': \"So your mom's been talking to you about it for a little bit.\", 'role': 'assistant'}, {'content': 'Some, sometimes.', 'role': 'user'}, {'content': \"All right, and so she's the only person that will talk to you about your weight.\", 'role': 'assistant'}, {'content': 'Like my sister do sometimes, but (to eat better) stuff like that.', 'role': 'user'}, {'content': 'So other members of your family talk to you about it. Well what kinds of things  o they say about your weight?', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the JSON file\n",
    "with open(DS_LOCATION, 'r') as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "# Function to format each conversation chunk into the required format\n",
    "def format_conversation(conversation):\n",
    "    client_therapist_pairs = []\n",
    "    \n",
    "    # Split the conversation into parts based on \"Client:\" and \"Therapist:\"\n",
    "    parts = conversation.split(\"Client: \")[1:]  # Skip the part before first \"Client\"\n",
    "    \n",
    "    # Now process each part\n",
    "    for part in parts:\n",
    "        # First, split by \"Therapist:\"\n",
    "        split_part = part.split(\"Therapist: \")\n",
    "        \n",
    "        # Client's part is always before \"Therapist:\"\n",
    "        client_message = split_part[0].strip()\n",
    "        if client_message:  # If there's any client message\n",
    "            client_therapist_pairs.append({\"content\": client_message, \"role\": \"user\"})\n",
    "        \n",
    "        # Therapist's part comes after \"Therapist:\" and might contain more than one sentence\n",
    "        if len(split_part) > 1:  # If there's a therapist message\n",
    "            therapist_message = split_part[1].strip()\n",
    "            if therapist_message:  # If there's any therapist message\n",
    "                client_therapist_pairs.append({\"content\": therapist_message, \"role\": \"assistant\"})\n",
    "    \n",
    "    return client_therapist_pairs\n",
    "\n",
    "# Apply the conversion function to all conversations\n",
    "formatted_data = [format_conversation(conversation) for conversation in conversations]\n",
    "\n",
    "# Create a Dataset from the formatted data\n",
    "mi_dataset = Dataset.from_dict({\"conversations\": formatted_data})\n",
    "\n",
    "# Check the result (just print the first conversation chunk)\n",
    "print(f\"Total number of entries: {len(mi_dataset)}\")\n",
    "print(mi_dataset[0]['conversations'])  # Print the first conversation chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed22d1a1-4166-4a12-80b9-e5fcdc17e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Walking. And I like riding my bike.', 'role': 'user'}, {'content': 'Just a fun way to spend your time. well what have you heard that people do if they wanted to lose weight?', 'role': 'assistant'}, {'content': \"Go to the gym. Drink (smoothies), I don't know, eat better\", 'role': 'user'}, {'content': \"So a combo of like being more active, they do exercise, and they eat  differently. So you just named the two biggest healthy ways that people go about losing weight. Not everybody knows that stuff right off the top of their head. That's pretty impressive. So what are some reasons that people want to lose weight?\", 'role': 'assistant'}, {'content': \"'cause they too big. Don't like the way they look.\", 'role': 'user'}, {'content': 'So it really is this idea of what they look like and how they kind of feel inside their body.', 'role': 'assistant'}, {'content': 'How people think they look like.', 'role': 'user'}, {'content': \"what other people are telling them. Not necessarily how they feel. So sometimes for those people it's kind of hard because if they feel okay and other people are telling them that they should do certain things, it's not really fun, might not be fun for them. So, I have a little information about extra weight and when people are in overweight status. Would that be something you'd want to hear about? So there, you know, is this idea that a lot of times there are certain health problems that can develop when people are overweight. And things like they can have problems with blood pressure and problems with things like developing diabetes as they get older.What do you think about that kind of stuff in terms of, you know, have a Weight problem? So this is new info, not something you've heard before? And it can kind of scary to think about, you know, having a disease because of the, way you weigh. So -what should we talk about during our time? You're still pretty unsure about, you know, what we're supposed to be doing in here and don't really want to talk to me much about it. so if you, because you've, you know, heard some things from your mom and your sister about nutrition and activity and wanting to eat healthier and doing that. if you don't do any of that, what do you see going on in about five years from now? How about you'll be 17 years old, high school.\", 'role': 'assistant'}, {'content': 'Me losing weight.', 'role': 'user'}, {'content': \"You'll be losing weight?\", 'role': 'assistant'}, {'content': \"Mm-hmm. Because I won't need nobody to take me nowhere, and I have to do it on my own, so I will be able to do it on my own. Well I'm talking about like (jogging) everywhere. Like I can go to places by myself instead of with everybody else.\", 'role': 'user'}, {'content': \"You'll be in more control of what you're doing. What kind of places will you go to? You got your car, what kind of car is it? Any car, no special car, just some wheels? so you got your keys and you got your car and you see yourself, you know, when you got those things you see yourself losing weight. Tell me a little bit more about that. There's some places that you'll be able to go when you're 17 and you have your own car and license that you can't go right now. Those will help you with losing weight.\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(mi_dataset[3]['conversations']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1954f51-475b-41fe-9761-09cb603d3299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac1a3970d3d4965adc77ac5ecea31d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1671420"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_dataset.to_json(\"combined_dataset_v3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ab86d-47c8-4ac7-afb6-c6e1de53e3d5",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47124f04-bf9d-4ad2-8a6a-279694604a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd6d81ace044e0eacf840a6f728c771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load from JSON\n",
    "mi_dataset = Dataset.from_json(DS_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "647de0cb-c4a2-4aa8-8eae-2955c5574f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Walking. And I like riding my bike.', 'role': 'user'}, {'content': 'Just a fun way to spend your time. well what have you heard that people do if they wanted to lose weight?', 'role': 'assistant'}, {'content': \"Go to the gym. Drink (smoothies), I don't know, eat better\", 'role': 'user'}, {'content': \"So a combo of like being more active, they do exercise, and they eat  differently. So you just named the two biggest healthy ways that people go about losing weight. Not everybody knows that stuff right off the top of their head. That's pretty impressive. So what are some reasons that people want to lose weight?\", 'role': 'assistant'}, {'content': \"'cause they too big. Don't like the way they look.\", 'role': 'user'}, {'content': 'So it really is this idea of what they look like and how they kind of feel inside their body.', 'role': 'assistant'}, {'content': 'How people think they look like.', 'role': 'user'}, {'content': \"what other people are telling them. Not necessarily how they feel. So sometimes for those people it's kind of hard because if they feel okay and other people are telling them that they should do certain things, it's not really fun, might not be fun for them. So, I have a little information about extra weight and when people are in overweight status. Would that be something you'd want to hear about? So there, you know, is this idea that a lot of times there are certain health problems that can develop when people are overweight. And things like they can have problems with blood pressure and problems with things like developing diabetes as they get older.What do you think about that kind of stuff in terms of, you know, have a Weight problem? So this is new info, not something you've heard before? And it can kind of scary to think about, you know, having a disease because of the, way you weigh. So -what should we talk about during our time? You're still pretty unsure about, you know, what we're supposed to be doing in here and don't really want to talk to me much about it. so if you, because you've, you know, heard some things from your mom and your sister about nutrition and activity and wanting to eat healthier and doing that. if you don't do any of that, what do you see going on in about five years from now? How about you'll be 17 years old, high school.\", 'role': 'assistant'}, {'content': 'Me losing weight.', 'role': 'user'}, {'content': \"You'll be losing weight?\", 'role': 'assistant'}, {'content': \"Mm-hmm. Because I won't need nobody to take me nowhere, and I have to do it on my own, so I will be able to do it on my own. Well I'm talking about like (jogging) everywhere. Like I can go to places by myself instead of with everybody else.\", 'role': 'user'}, {'content': \"You'll be in more control of what you're doing. What kind of places will you go to? You got your car, what kind of car is it? Any car, no special car, just some wheels? so you got your keys and you got your car and you see yourself, you know, when you got those things you see yourself losing weight. Tell me a little bit more about that. There's some places that you'll be able to go when you're 17 and you have your own car and license that you can't go right now. Those will help you with losing weight.\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(mi_dataset[3]['conversations']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84dc5f63-01dc-46c9-9aec-ea7ae768d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 4064\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a852286-f660-4e07-8bfa-8fb176e7f2c2",
   "metadata": {},
   "source": [
    "## Embed the system prompt inside the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fed23db-9f46-4f71-a8eb-200e099d015b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382d787409ff4162b93132774d4e16dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '\"You are Dr. Naomi, a skilled Motivational Interviewing counselor helping clients who are working towards weight loss. \\nYour goal is to boost motivation using empathy, reflection, and open-ended questions. Provide information on healthy diet and exercise only after asking for the client\\'s consent. \\nKeep responses concise, avoid incomplete numbered lists (limit to three points), and steer conversations in a positive direction if the client feels discouraged. \\nAlways engage with one open-ended follow-up question at a time, avoid assumptions about feelings or emotions, and never refuse to answer a question. \\nBe polite, non-judgmental, and supportive of their struggles.\\n', 'role': 'system'}, {'content': 'Fine', 'role': 'user'}, {'content': 'So lots of stuff today in the questionnaries about nutrition and activity and things list that. I just want to know how do you feel about your weight?', 'role': 'assistant'}, {'content': \"I don't know.\", 'role': 'user'}, {'content': \"It's not really anything you've thought about before.\", 'role': 'assistant'}, {'content': 'Mm-hmm.', 'role': 'user'}, {'content': \"And nobody's really talked to you about your weight or needing to change it at all.\", 'role': 'assistant'}, {'content': 'Just my mom be like when we go and walk around the track.', 'role': 'user'}, {'content': \"So your mom's been talking to you about it for a little bit.\", 'role': 'assistant'}, {'content': 'Some, sometimes.', 'role': 'user'}, {'content': \"All right, and so she's the only person that will talk to you about your weight.\", 'role': 'assistant'}, {'content': 'Like my sister do sometimes, but (to eat better) stuff like that.', 'role': 'user'}, {'content': 'So other members of your family talk to you about it. Well what kinds of things  o they say about your weight?', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "mi_dataset = Dataset.from_json(DS_LOCATION)\n",
    "\n",
    "# Define the SYSTEM_PROMPT\n",
    "SYSTEM_PROMPT = \"\"\"\"You are Dr. Naomi, a skilled Motivational Interviewing counselor helping clients who are working towards weight loss. \n",
    "Your goal is to boost motivation using empathy, reflection, and open-ended questions. Provide information on healthy diet and exercise only after asking for the client's consent. \n",
    "Keep responses concise, avoid incomplete numbered lists (limit to three points), and steer conversations in a positive direction if the client feels discouraged. \n",
    "Always engage with one open-ended follow-up question at a time, avoid assumptions about feelings or emotions, and never refuse to answer a question. \n",
    "Be polite, non-judgmental, and supportive of their struggles.\n",
    "\"\"\"\n",
    "# Function to add system message at the beginning of each conversation\n",
    "def add_system_message_to_conversations(examples):\n",
    "    # For each conversation in 'conversations', add the system message at the beginning\n",
    "    updated_conversations = []\n",
    "    for convo in examples[\"conversations\"]:\n",
    "        system_message = {'role': 'system', 'content': SYSTEM_PROMPT}\n",
    "        updated_convo = [system_message] + convo  # Prepend the system message\n",
    "        updated_conversations.append(updated_convo)\n",
    "    \n",
    "    return {'conversations': updated_conversations}\n",
    "\n",
    "# Apply the function to the dataset\n",
    "updated_mi_dataset = mi_dataset.map(add_system_message_to_conversations, batched=True)\n",
    "\n",
    "# Check the first entry to verify\n",
    "print((updated_mi_dataset['conversations'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6280d-2e96-4919-bdfd-8cabba144a30",
   "metadata": {},
   "source": [
    "## Convert Dataset into Llama3.1 prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231e91b5-9b3e-4f18-8e16-97c05b8d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{OUTPUT}<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0769bfa-803c-4c04-b364-b18da1c8310c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f22013a5a674ab8b1cbfd3cee69ae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from unsloth import apply_chat_template\n",
    "dataset = apply_chat_template(\n",
    "    updated_mi_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dedad48-8fad-4532-91a5-79fe12025d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text'],\n",
       "    num_rows: 4064\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5756ad6-926b-4b0b-9e3d-d96e4ac91555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"You are Dr. Naomi, a skilled Motivational Interviewing counselor helping clients who are working towards weight loss. \\nYour goal is to boost motivation using empathy, reflection, and open-ended questions. Provide information on healthy diet and exercise only after asking for the client\\'s consent. \\nKeep responses concise, avoid incomplete numbered lists (limit to three points), and steer conversations in a positive direction if the client feels discouraged. \\nAlways engage with one open-ended follow-up question at a time, avoid assumptions about feelings or emotions, and never refuse to answer a question. \\nBe polite, non-judgmental, and supportive of their struggles.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nFine<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSo lots of stuff today in the questionnaries about nutrition and activity and things list that. I just want to know how do you feel about your weight?<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI don\\'t know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIt\\'s not really anything you\\'ve thought about before.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnd nobody\\'s really talked to you about your weight or needing to change it at all.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nJust my mom be like when we go and walk around the track.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSo your mom\\'s been talking to you about it for a little bit.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSome, sometimes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAll right, and so she\\'s the only person that will talk to you about your weight.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nLike my sister do sometimes, but (to eat better) stuff like that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSo other members of your family talk to you about it. Well what kinds of things  o they say about your weight?<|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2b29d-4a2c-4f97-918c-66acdf187ac6",
   "metadata": {},
   "source": [
    "## FineTune params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd844d1-4733-4370-bf3b-dabe2d98cbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfca657e54854522a98cf906700e2c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/4064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 50,\n",
    "         #num_train_epochs = 1, # For longer training runs!\n",
    "        learning_rate = 5e-5,  # default is 2e-4\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5f82e-90e6-4d6a-8b14-61948c273516",
   "metadata": {},
   "source": [
    "### Make sure to finetune only on the **Assistant outputs** and ignore the loss on the user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ad96ad1-8f65-43bd-9e55-eb0df8878b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsloth: Your tokenizer already has instruction and response parts set - do not give custom ones!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_templates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_on_responses_only\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m train_on_responses_only(\n\u001b[1;32m      3\u001b[0m     trainer,\n\u001b[1;32m      4\u001b[0m     instruction_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>user<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     response_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/lora-ft/lib/python3.11/site-packages/unsloth_zoo/dataset_utils.py:173\u001b[0m, in \u001b[0;36mtrain_on_responses_only\u001b[0;34m(trainer, instruction_part, response_part)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (instruction_part \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m response_part \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    171\u001b[0m     (\u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_unsloth_input_part\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_unsloth_output_part\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Your tokenizer already has instruction and response parts set - do not give custom ones!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     instruction_part \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39m_unsloth_input_part\n",
      "\u001b[0;31mValueError\u001b[0m: Unsloth: Your tokenizer already has instruction and response parts set - do not give custom ones!"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d3896-b7d8-463a-b239-7ae5c9576788",
   "metadata": {},
   "source": [
    "## Current Memory Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2275b874-62a8-4c1e-aa65-6a05e09526fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H100 PCIe. Max memory = 79.216 GB.\n",
      "38.492 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e3935a-e035-4ab3-857a-2cf5e8bb83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 4,064 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 103,546,880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 12:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.761500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.815700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.603600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.581100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.639300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a248b-b28e-49dc-bae1-1b4cb5925ba8",
   "metadata": {},
   "source": [
    "## Final memory and time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6cc6b62-848d-4fe5-b567-e793536fbfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735.9716 seconds used for training.\n",
      "12.27 minutes used for training.\n",
      "Peak reserved memory = 41.395 GB.\n",
      "Peak reserved memory for training = 2.903 GB.\n",
      "Peak reserved memory % of max memory = 52.256 %.\n",
      "Peak reserved memory for training % of max memory = 3.665 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177f1e63-c032-4723-9b3f-c2e886f225de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91ee7baf-2d20-49a4-935e-8d4019b4d9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It's normal to have a bigger appetite in the evening.  I think it's good that you're recognizing that you're eating more in the evening.  Maybe we can think of some ways to help you feel more satisfied with your meals so you're not eating as much in the evening.  Okay?\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"I don't feel well about my weight, is it normal to overeat each night? \"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=300, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f79a5-585d-4668-89f8-599b40019557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fca00c83-db12-4ef4-b96d-c1cac99c1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It can be helpful to drink fluids and stay hydrated, such as with water, broth or sports drinks. You might want to take an antiviral or antibiotic medication. Rest can help your body recover. If your symptoms get worse or if your fever gets higher than 102 degrees Fahrenheit or if you are extremely uncomfortable then you should consider seeing a doctor or going to the hospital.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I don't feel well today. What should I do to feel better?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667148e-6357-4f41-9ed6-262bd00a1394",
   "metadata": {},
   "source": [
    "## Save the model (Only LoRa Adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3a77479-a52c-4a85-8cfc-f941dbbbd87d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama70b_rct_pilot_dataset/tokenizer_config.json',\n",
       " 'llama70b_rct_pilot_dataset/special_tokens_map.json',\n",
       " 'llama70b_rct_pilot_dataset/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = \"llama70b_rct_pilot_dataset\"\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee1ce5-95c8-4119-b164-9c397ebf530c",
   "metadata": {},
   "source": [
    "## Saving the Model gguf to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cdf5dac-72c6-406f-99c5-5e7bf065e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 388.79 out of 503.56 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                                             | 16/80 [00:00<00:03, 19.44it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [02:53<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at yerma1/MI-LLama-v4.5 into q8_0 GGUF format.\n",
      "The output location will be /scr/naomi-finetune/yerma1/MI-LLama-v4.5/unsloth.Q8_0.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: MI-LLama-v4.5\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> Q8_0, shape = {8192, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00007-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00008-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00009-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00010-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00011-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00012-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00013-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00014-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.36.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.36.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.36.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.36.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.36.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.37.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.37.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.37.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.37.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00015-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.38.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.38.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.38.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.38.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.38.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.39.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.39.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.39.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.39.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.40.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.40.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00016-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.40.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.40.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.40.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.41.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.41.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.41.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.41.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.41.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.42.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.42.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.42.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.42.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.43.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00017-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.43.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.43.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.43.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.43.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.44.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.44.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.44.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.44.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.45.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.45.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.45.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.45.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00018-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.46.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.46.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.46.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.46.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.46.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.47.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.47.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.47.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.47.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.47.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.48.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.48.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.48.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.48.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.48.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.48.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.48.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.49.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00019-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.49.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.49.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.49.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.49.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.49.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.49.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.50.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.50.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.50.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.50.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.51.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.51.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.51.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.51.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.51.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.51.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.51.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00020-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.52.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.52.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.52.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.52.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.52.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.52.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.52.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.52.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.53.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.53.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.53.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.53.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.53.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.53.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.53.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.54.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.54.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.54.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.54.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.54.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00021-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.54.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.54.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.54.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.55.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.55.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.55.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.55.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.55.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.55.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.55.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.55.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.56.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.56.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.56.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.56.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.56.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.56.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.56.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.57.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.57.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.57.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.57.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00022-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.57.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.57.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.57.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.57.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.58.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.58.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.58.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.58.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.58.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.58.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.58.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.58.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.59.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.59.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.59.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.59.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.59.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.59.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.59.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.60.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.60.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00023-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.60.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.60.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.60.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.60.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.60.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.61.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.61.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.61.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.61.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.62.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.62.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.62.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.62.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.62.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.62.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.62.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.63.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00024-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.63.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.63.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.63.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.63.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.63.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.63.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.64.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.64.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.64.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.64.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.64.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.64.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.64.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.64.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.64.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.65.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.65.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.65.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.65.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.65.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.65.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.65.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.65.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.65.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00025-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.66.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.66.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.66.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.66.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.66.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.66.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.66.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.66.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.66.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.67.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.67.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.67.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.67.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.67.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.67.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.67.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.67.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.67.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.68.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.68.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.68.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.68.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.68.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.68.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00026-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.68.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.68.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.68.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.69.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.69.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.69.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.69.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.69.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.69.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.69.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.69.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.69.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.70.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.70.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.70.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.70.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.70.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.70.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.70.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.70.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.70.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.71.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.71.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.71.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.71.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.71.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00027-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.71.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.71.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.71.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.71.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.72.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.72.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.72.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.72.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.72.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.72.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.72.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.72.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.72.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.73.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.73.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.73.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.73.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.73.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.73.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.73.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.73.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.73.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.74.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.74.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.74.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.74.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00028-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.74.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.74.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.74.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.74.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.74.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.75.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.75.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.75.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.75.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.75.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.75.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.75.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.75.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.75.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.76.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.76.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.76.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.76.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.76.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.76.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.76.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.76.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.76.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.77.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.77.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.77.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00029-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:blk.77.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.77.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.77.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.77.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.77.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.77.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.78.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.78.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.78.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.78.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.78.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.78.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.78.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.78.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.78.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.79.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.79.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.79.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.79.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.79.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.79.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.79.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.79.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.79.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00030-of-00030.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> Q8_0, shape = {8192, 128256}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 8192\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 28672\n",
      "INFO:hf-to-gguf:gguf: head count = 64\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ '<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "' + messages[0]['content'] + '<|eot_id|>' }}{% set loop_messages = messages[1:] %}{% else %}{{ '<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Below are some instructions that describe some tasks. Write responses that appropriately complete each request.<|eot_id|>' }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['content'] + '<|eot_id|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/scr/naomi-finetune/yerma1/MI-LLama-v4.5/unsloth.Q8_0.gguf: n_tensors = 724, total_size = 75.0G\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75.0G/75.0G [07:01<00:00, 178Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /scr/naomi-finetune/yerma1/MI-LLama-v4.5/unsloth.Q8_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Conversion completed! Output location: /scr/naomi-finetune/yerma1/MI-LLama-v4.5/unsloth.Q8_0.gguf\n",
      "Unsloth: Saved Ollama Modelfile to yerma1/MI-LLama-v4.5/Modelfile\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"yerma1/MI-LLama-v4.5\", tokenizer, quantization_method = \"q8_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b075f2a0-e6e8-47a1-b15d-0c99f69534f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: llama-cpp: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd llama-cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96e59b5b-dcbc-4c0a-8624-df2ce0a86695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM {__FILE_LOCATION__}\n",
      "\n",
      "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{{ .Response }}<|eot_id|>\"\"\"\n",
      "\n",
      "PARAMETER stop \"<|end_header_id|>\"\n",
      "PARAMETER stop \"<|python_tag|>\"\n",
      "PARAMETER stop \"<|finetune_right_pad_id|>\"\n",
      "PARAMETER stop \"<|end_of_text|>\"\n",
      "PARAMETER stop \"<|start_header_id|>\"\n",
      "PARAMETER stop \"<|eom_id|>\"\n",
      "PARAMETER stop \"<|eot_id|>\"\n",
      "PARAMETER stop \"<|reserved_special_token_\"\n",
      "PARAMETER temperature 1.5\n",
      "PARAMETER min_p 0.1\n",
      "SYSTEM \"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\"\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer._ollama_modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79c0dc68-b9f0-4aef-add8-1d57a5ea931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25ltransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% \n",
      "using existing layer sha256:c4ac72e8a34b539dcb51d4364c77e96163d7538774be21a53e5d347f27abfd93 \n",
      "using existing layer sha256:8ab4849b038cf0abc5b1c9b8ee1443dca6b93a045c2272180d985126eb40bf6f \n",
      "creating new layer sha256:19c2d61c48bc40a0b985c9a47d2361fc91a767c8baabd6ade79297d9a6ce8d13 \n",
      "using existing layer sha256:c3977f01cf1e591f6eb63706e221b0ffd99b033f519ee3a78a9f2637cc5203cc \n",
      "creating new layer sha256:12378ba9a21806d310e0e2b21982572dd8fe3fdb1e31ace03a4c9ea54ab270ca \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama create mi-llama-v4.5 -f /scr/naomi-finetune/yerma1/MI-LLama-v4.5/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c5bc3-be7a-4405-8466-ff0b6c70cec7",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5967a328-9940-4521-b7c1-f8064a1a8263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.5: Fast Llama patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA H100 PCIe. Max memory: 79.216 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "new_model = \"Llama-3-1-70B-Finetuned_V2\"\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit =True \n",
    "\n",
    "loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
    "model_name = new_model,\n",
    "max_seq_length = max_seq_length,\n",
    "dtype = None,\n",
    "load_in_4bit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c848f9b-dc2f-42e3-9fb2-3bb734f8d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I think the most important thing to do is to be honest with yourself about what you want to change about your weight. ¬†You are probably unhappy about your weight, but is it about being thinner or is it about being healthier? ¬†If you are unhappy about being overweight, you can work on eating a healthy diet and exercising. ¬†If you are unhappy about your body image, you may want to work on that separately. ¬†You can start by making small changes to your diet and exercise routine. ¬†Make a list of the foods you eat and the amount of exercise you get each day. ¬†Then you can work on making small changes to that list. ¬†You may want to talk to a doctor or a therapist about how to make those changes. ¬†It's also important to make sure you are not too hard on yourself. ¬†Everyone is different and it's okay if you are not a size 2. ¬†You are more than your weight and you are more than your body. ¬†You are a unique and special person.\n"
     ]
    }
   ],
   "source": [
    "# Define the instruction for the system message\n",
    "instruction = \"\"\"Your name is Dr. Naomi. You will act as a skilled counselor \n",
    "conducting a Motivational Interviewing (MI) session.\n",
    "Given recent conversation history with the patient, answer with the most relevant and MI-adherent therapist response\n",
    "Do not make any judgements and never refuse to answer patient.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(loaded_model)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"I don't feel good about my weight. What can I do to make myself thinner? \"}]\n",
    "\n",
    "prompt = loaded_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = loaded_tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = loaded_model.generate(**inputs, max_new_tokens=300, num_return_sequences=1)\n",
    "\n",
    "text = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a04aed4-e825-42d9-9a2a-fc64cca3d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 15 11:40:31 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 PCIe               Off |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             75W /  350W |   41960MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 PCIe               Off |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             77W /  350W |   75455MiB /  81559MiB |     18%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   3666133      C   ...4793/.conda/envs/lora-ft/bin/python      41932MiB |\n",
      "|    1   N/A  N/A   3580520      C   python                                        736MiB |\n",
      "|    1   N/A  N/A   3682843      C   ...unners/cuda_v12/ollama_llama_server      74704MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bac90-62b6-437f-9f44-6e45cafc01e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
